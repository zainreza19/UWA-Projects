---
title: "Project 2"
author: "ZAIN REZA HASAN - 24077182; BENJAMIN JOVEL - 23995719"
date: "2023-10-05"
output: html_document
---

# Introduction

The “Global YouTube Statistics 2023” dataset refers to a collection of data that provides information and insights about YouTube videos, channels, and user engagement on a global scale. This dataset can include a wide range of variables and metrics related to videos, channels, viewership, interactions, etc.

In our project we are using three datasets namely youtube_data, GDP_data, and internet_usage_data. 



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Importing Datasets}

gdp_data = read.csv("GDP.csv")

yt_data = read.csv("Global YouTube Statistics (1).csv")

int_data = read.csv("internet_usage.csv")
```
Here we are merging three datasets using merge function
```{r Joining}

#Joining YT data and GDP data of 2022
m2 <- merge(yt_data,gdp_data,by.x = 'Country',by.y='Country.Name')
m2

m2 <- merge(m2,int_data, by.x ='Country', by.y = 'Country.or.Area')
m2

#Renaming the Columns
library(dplyr)
m2 <- m2 %>% 
  rename(GDP_2022=X2022)

```
```{r}

# Removing the irrelavant columns from the dataframe

m2$rank <- NULL
m2$Youtuber <- NULL
m2$Title <- NULL
m2$Abbreviation <- NULL
m2$created_month <- NULL
m2$Country.Code <- NULL
m2$x <- NULL

# subsetting the columns that are numerical to plot a correlation

# Assuming df is your dataframe
numeric_cols <- m2[sapply(m2, is.numeric)]

options(scipen = 999)






```

```{r}
install.packages("skimr")
library(skimr)
skim(m2)
```
```{r Removing for Coorelation}

corr_clean <- na.omit(numeric_cols)
skim(corr_clean)


```

```{r}
install.packages("corrplot")  
library(corrplot)

correlation_matrix <- cor(corr_clean)
#print(correlation_matrix)
corrplot(correlation_matrix, method = "color",tl.cex = 0.7)


```


```{r Removing Columns based on high coorelation}
#Removing since all of them are correlated | lowest_monthly_earnings, highest_monthly_earnings lowest_yearly_earnings

library(dplyr)
m2 <- m2 %>%
  select(-lowest_monthly_earnings,-highest_monthly_earnings,-lowest_yearly_earnings)

```


```{r Finding the median of Earnings}


# Find the median of the 'highest_yearly_earnings' column
median_value <- median(m2$highest_yearly_earnings)

# Print the median value
print(median_value)

# Create a new binary column 'earnings_binary' based on the median
m2$earnings_binary <- ifelse(m2$highest_yearly_earnings > median_value, 1, 0)

# Print the updated dataset
print(m2)
```

```{r More Cleaning}


#Removing since all of them are correlated | lowest_monthly_earnings, highest_monthly_earnings lowest_yearly_earnings, created_date

library(dplyr)
m2 <- m2 %>%
  select(-subscribers_for_last_30_days)

m2$category <- gsub("nan", "Other", m2$category)
m2 <- na.omit(m2)

#rename a column

colnames(m2)[colnames(m2) == "Gross.tertiary.education.enrollment...."] <- "educationEnrollmentTertiary"



head(m2)

```


```{r Splitting our Data into Test and Training Data}

set.seed(11051998) # setting a seed code
m2$rNum <- runif(dim(m2)[1]) # making a random number column
m2Train <- subset(m2,rNum<=0.8) # making a training set of 80/20 split
m2Test <- subset(m2,rNum>0.8) # making a testing set


```


```{r Building up a set inside Training set for Calibration/Validation}

validationUse <- rbinom(n=dim(m2Train)[1],size=1,prob=0.1)>0 # generates a random bionomial distribution 1 or 0s with probability 10%
m2Cal <- subset(m2Train,validationUse) # subsets 10% of the data from the training set we made above
m2Train <- subset(m2Train, !validationUse) # puts the remaining 90% back in our dataset


```


```{r Building up a Decision Tree}

library('rpart')

# (features <- paste(m2Train$earnings_binary,'> 0 ~ ',paste(c(m2Train$catVars, m2Train$numericVars), collapse=' + '), sep=''))
#features <- as.formula(features)

tmodel <- rpart(earnings_binary ~ Country+ subscribers+ video.views+ category+ uploads+ channel_type+ video_views_rank+ country_rank+ channel_type_rank+ created_year+ created_date+educationEnrollmentTertiary+ Population.x+ Unemployment.rate+ Urban_population+ Latitude+ Longitude+ GDP_2022+ Internet.Users+ Population.y+ Rank+ Percentage+ Rank.1+ rNum+ category, data=m2Train)



```
```{r Plot}

library(rpart)
library(rpart.plot)
rpart.plot(tmodel)


```

```{r Calculating AUC}
install.packages("pROC")
library(pROC)
print(roc(predict(tmodel, newdata=m2Train), m2Train[,earnings_binary]))
```

# Part 02 - Clustering 
```{r}
# More data transformation to implement clustering. Removing irrelevant columns that doesn't contribute for modelling 

m2$country_rank <- NULL
m2$channel_type_rank <- NULL
m2$video_views_for_the_last_30_days <-  NULL
m2$subscribers_for_last_30_days <- NULL
m2$created_year <- NULL
m2$created_date <- NULL
m2$Latitude <- NULL
m2$Longitude <- NULL
m2$X <- NULL
m2$Population.x <- NULL
m2$Population.y <- NULL
m2$Rank.1 <- NULL
m2$Rank <- NULL
m2$channel_type <- NULL
m2$Percentage <- NULL
m2$category <- NULL

m2$Internet.Users <- NULL
```
```{r}
# Install and Load Necessary Libraries
install.packages("ggplot2")
install.packages("cluster")
install.packages("factoextra")
library(ggplot2)
library(cluster)
library(factoextra)
```
```{r}
# Feature selection to implement KMeans Clustering
data <- m2[, c("subscribers", "video.views", "uploads", "Unemployment.rate", "Urban_population", "GDP_2022")]
```
```{r}
# Scaling the data
data_scaled <- scale(data)
```
```{r Data transformation}
# Dealing with NA/Nan/ Inf values in the dataframe 
sum(is.na(data_scaled))
sum(is.infinite(data_scaled))

# Removing those missing values 
data_scaled <- data_scaled[complete.cases(data_scaled), ]

```

```{r}
# Finding the optimal number of clusters using the Elbow Method
wss <- sapply(1:10, function(k){kmeans(data_scaled, centers=k, nstart=25)$tot.withinss})
```
```{r}
# We use Elbow Method Plot to findout the number of clusters
plot(1:10, wss, type="b", 
     main="Elbow Method", 
     xlab="Number of Clusters", 
     ylab="Within-groups sum of squares")
```
From the curve, the "elbow" seems to be around the number 4 on the x-axis. This means that increasing the number of clusters beyond 4 might not result in significant improvements in clustering compactness. Thus, 4 could be an optimal number of clusters for this data

```{r}
# Implementation of clustering, we will choose k = 4
set.seed(123)
kmeans_result <- kmeans(data_scaled, centers=4, nstart=25)
```

```{r}
# We visualize the clusters
fviz_cluster(kmeans_result, data = data_scaled)
```
From the plot we can make some of the observations as stated below:

1. Cluster Sizes & Densities:

Cluster 1 (Red): This cluster seems densely packed with many data points close to each other, suggesting high similarity among the data points in this group.
Cluster 2 (Green): Also densely populated, but the spread is slightly more than the red cluster.
Cluster 3 (Light Blue): A less dense cluster with data points more spread out. There might be subgroups within this cluster, as suggested by the smaller dense regions.
Cluster 4 (Purple): This cluster has the fewest data points, which are quite spread out. These might be outliers or data points that didn't fit well into the other clusters.

The data has been grouped into four clusters based on their similarities across the two primary dimensions. Clusters 1 and 2 are densely populated and relatively close, suggesting more commonalities between them compared to Clusters 3 and 4. The latter two are more spread out, with Cluster 4 potentially capturing outliers or unique cases in the dataset. Both dimensions (Dim1 and Dim2) play a substantial role in the clustering, with Dim1 having a slightly higher contribution to the variance.



